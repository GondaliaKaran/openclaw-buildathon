# Adaptive Vendor Evaluation Agent

**OpenClaw Buildathon ‚Äî Challenge #104**

Research like a team of 5 analysts, decide like a CTO ‚Äî adaptive tech evaluation with dynamic criteria weighting.

## What This Does

An AI agent that evaluates vendors/tools/platforms with **adaptive criteria weighting**. Unlike static comparison matrices, this agent changes its evaluation criteria based on what it discovers during research.

**Example**: Discovering a vendor had 3 outages last month automatically increases the "Uptime/Reliability" weight and triggers deeper investigation of all vendors' status pages.

## Architecture (Three-Layer Breakdown)

### Interface Layer (SOUL.md)
- Senior tech evaluator/CTO advisor persona
- Produces structured comparison with full reasoning chain
- 8-section mandatory output format ensuring every evaluation is thorough

### Logic Layer (Python Orchestrator)
- **Candidate Identifier** (30 min) ‚Äî Finds 3-5 relevant vendors based on context
- **Multi-Criteria Researcher** (1 hr) ‚Äî Deep investigation across technical, operational, business, and hidden risk dimensions
- **Dynamic Weight Adjuster** (45 min) ‚Äî Reshapes criteria weights based on research discoveries
- **Recommendation Synthesizer** (30 min) ‚Äî Produces final comparison with justified recommendation

### Integration Layer (ClawHub + OpenAI)
- ClawHub web search for vendor sites, GitHub, G2, status pages, compliance registries
- OpenAI GPT-4o for analysis and synthesis

## Key Deliverables

### ‚úÖ Accepts evaluation request and autonomously identifies candidates
The agent extracts context (category, tech stack, domain, region, scale) from natural language queries and identifies 3-5 relevant candidates with rationale for each inclusion.

### ‚úÖ Dynamic criteria re-weighting (3+ instances per evaluation)
Every evaluation shows explicit weight adjustment tables:

| Criterion | Initial Weight | Final Weight | Reason for Change |
|-----------|---------------|--------------|-------------------|
| Payment Success Rate | 20% | 28% | Razorpay status page: 4 UPI incidents in 90 days |
| Pricing/MDR | 10% | 18% | Stripe USD pricing = 3.4% effective vs Razorpay 2% |
| Vendor Health | 5% | 12% | PayU parent Prosus restructured fintech division |

### ‚úÖ Structured comparison with reasoning chain
Full output includes: context summary ‚Üí candidates ‚Üí discoveries ‚Üí weight table ‚Üí comparison matrix ‚Üí hidden risks ‚Üí recommendation ‚Üí reproducibility notes.

### ‚úÖ Same category, different context = different evaluation
"Payment gateways for Indian startup" produces completely different weights, scores, and recommendations than "Payment gateways for US enterprise" ‚Äî demonstrated with different starting weight templates, different vendor pools, and different discoveries.

## üîç Research Methodology: What the Agent Actually Scans

The agent performs **real-time internet research** across 7 data dimensions for each vendor. Here's what it searches and fetches:

### 1. Technical Health & Quality
**What it checks:**
- GitHub repository metrics (stars, forks, issues, last commit)
- SDK/library availability and maintenance
- Documentation quality and completeness
- API design patterns and developer experience

**Example searches per scenario:**

**Payment Gateway:**
```
‚Ä¢ "razorpay github python SDK"
  ‚Üí Finds: github.com/razorpay/razorpay-python
‚Ä¢ Fetch: https://api.github.com/repos/razorpay/razorpay-python
  ‚Üí Gets: 487 stars, last push 12 days ago, 23 open issues
‚Ä¢ "stripe API documentation quality review"
‚Ä¢ Fetch: https://docs.stripe.com/api
```

**CDN:**
```
‚Ä¢ "cloudflare terraform provider github"
  ‚Üí Finds: github.com/cloudflare/terraform-provider-cloudflare
‚Ä¢ Fetch: https://api.github.com/repos/cloudflare/terraform-provider-cloudflare/contributors
  ‚Üí Bus factor check: top 3 contributors = 67% of commits
‚Ä¢ "fastly edge compute documentation"
```

**Observability:**
```
‚Ä¢ "datadog agent github stars"
‚Ä¢ "prometheus exporter ecosystem npm"
‚Ä¢ "grafana plugin marketplace size"
```

---

### 2. Reliability & Uptime History
**What it checks:**
- Status page incident history (last 90 days)
- Current service health across regions
- Planned maintenance windows
- SLA claims vs actual performance

**Example searches per scenario:**

**Payment Gateway:**
```
‚Ä¢ "razorpay status page"
  ‚Üí Finds: status.razorpay.com
‚Ä¢ Fetch: https://status.razorpay.com/api/v2/incidents.json
  ‚Üí 4 UPI-related incidents in last 90 days
‚Ä¢ "cashfree outage 2025 site:twitter.com"
  ‚Üí Social signals for unreported incidents
```

**CDN:**
```
‚Ä¢ Fetch: https://www.cloudflarestatus.com/api/v2/summary.json
  ‚Üí Real-time PoP health: Mumbai operational, Ahmedabad under_maintenance
‚Ä¢ "akamai cdn outage incident 2025 2026"
‚Ä¢ "fastly status page api"
```

**Observability:**
```
‚Ä¢ "datadog downtime incident 2025"
‚Ä¢ "new relic status page API"
‚Ä¢ "grafana cloud reliability SLA"
```

---

### 3. Compliance & Security Certifications
**What it checks:**
- PCI DSS, SOC 2, ISO 27001 status
- Region-specific compliance (RBI in India, GDPR in EU, HIPAA in US)
- Security audit reports and attestations
- Data residency and sovereignty claims

**Example searches per scenario:**

**Payment Gateway:**
```
‚Ä¢ "razorpay PCI DSS level compliance certificate"
‚Ä¢ "stripe SOC2 report 2025"
‚Ä¢ "cashfree RBI payment aggregator license"
  ‚Üí Critical for India operations
‚Ä¢ Fetch: https://razorpay.com/security
  ‚Üí Claims: PCI DSS Level 1, ISO 27001
```

**CDN:**
```
‚Ä¢ "cloudflare GDPR compliance data residency"
‚Ä¢ "akamai FedRAMP certification government"
‚Ä¢ "fastly HIPAA BAA business associate agreement"
```

**Observability:**
```
‚Ä¢ "datadog SOC2 Type II audit report"
‚Ä¢ "new relic HIPAA compliance healthcare"
‚Ä¢ "grafana ISO 27001 certificate"
```

---

### 4. Pricing Structure & Hidden Costs
**What it checks:**
- Base pricing tiers and volume breakpoints
- Regional pricing variations (India vs US vs EU)
- Hidden fees (setup, support, API calls, bandwidth)
- Non-linear cost scaling (10x traffic ‚â† 10x cost)

**Example searches per scenario:**

**Payment Gateway:**
```
‚Ä¢ "razorpay pricing india MDR 2025"
‚Ä¢ Fetch: https://razorpay.com/pricing-india
  ‚Üí 2% for UPI, 2.5% for cards
‚Ä¢ "stripe pricing currency conversion fees"
‚Ä¢ Calculate: 10K tx/month at ‚Çπ500 avg ‚Üí ‚Çπ1,00,000 vs $2,900
  ‚Üí At 100K tx/month ‚Üí non-linear jump due to forex
```

**CDN:**
```
‚Ä¢ Fetch: https://www.fastly.com/pricing
  ‚Üí India region: $0.28/GB (100GB-10TB)
  ‚Üí Constraint: "No more than 10% traffic from India" on packages
‚Ä¢ "cloudflare enterprise pricing negotiation"
‚Ä¢ Calculate: 1TB ‚Üí 3TB ‚Üí 10TB to find pricing cliffs
```

**Observability:**
```
‚Ä¢ "datadog pricing calculator ingestion cost"
‚Ä¢ "grafana cloud pricing per user vs self-hosted"
‚Ä¢ "prometheus managed service cost comparison"
```

---

### 5. User Sentiment & Real Experiences
**What it checks:**
- G2/Capterra aggregate ratings and review count
- Reddit/HN discussions about pain points
- Common complaints (support, billing surprises, integration complexity)
- Migration stories (switching from competitor)

**Example searches per scenario:**

**Payment Gateway:**
```
‚Ä¢ "razorpay G2 reviews 2025"
  ‚Üí 4.3/5 from 830 reviews
‚Ä¢ "stripe review site:reddit.com"
  ‚Üí Common themes: great docs, expensive at scale
‚Ä¢ "cashfree integration problems issues"
```

**CDN:**
```
‚Ä¢ "cloudflare vs fastly comparison site:news.ycombinator.com"
‚Ä¢ "akamai customer support review G2"
‚Ä¢ "vercel edge network performance reddit"
```

**Observability:**
```
‚Ä¢ "datadog vs new relic cost comparison 2025"
‚Ä¢ "grafana cloud review site:reddit.com"
‚Ä¢ "self-hosted prometheus challenges"
```

---

### 6. Corporate Stability & Risk Signals
**What it checks:**
- Recent acquisitions or mergers
- Funding rounds and runway estimates
- Layoffs or restructuring announcements
- Executive turnover (CEO changes)
- Product sunset/pivot signals

**Example searches per scenario:**

**Payment Gateway:**
```
‚Ä¢ "payu acquisition prosus restructuring 2025"
  ‚Üí Finds: Parent company Prosus restructured fintech division
‚Ä¢ "cashfree funding series round 2025 2026"
‚Ä¢ "stripe IPO layoffs 2025"
```

**CDN:**
```
‚Ä¢ "fastly acquisition rumors 2025"
‚Ä¢ "cloudflare layoffs workforce reduction"
‚Ä¢ "akamai merger news 2026"
```

**Observability:**
```
‚Ä¢ "datadog acquisition splunk 2025"
‚Ä¢ "new relic layoffs restructuring"
‚Ä¢ "grafana labs funding series"
```

---

### 7. Technology Risk & Lock-in
**What it checks:**
- API deprecation schedules and breaking changes
- SDK maintenance status
- Data export capabilities (lock-in risk)
- Proprietary formats vs open standards
- Migration complexity from vendor

**Example searches per scenario:**

**Payment Gateway:**
```
‚Ä¢ "stripe API deprecation sunset 2025"
‚Ä¢ "razorpay webhook format change breaking"
‚Ä¢ Search GitHub issues: repo:razorpay/razorpay-python label:breaking
‚Ä¢ "payment gateway migration guide switching"
```

**CDN:**
```
‚Ä¢ "cloudflare workers API breaking changes"
‚Ä¢ "fastly VCL deprecation timeline"
‚Ä¢ "migrating from cloudflare to fastly"
```

**Observability:**
```
‚Ä¢ "datadog agent version EOL support"
‚Ä¢ "prometheus remote write API changes"
‚Ä¢ "grafana dashboard export migration"
```

---

## Research Depth: Numbers

For a typical evaluation with 4 candidates:
- **40-60 web searches** (10-15 per vendor)
- **20-30 URL fetches** (GitHub API, status pages, pricing pages)
- **15-20 specific evidence citations** in the final report
- **All vendors researched equally** (no "N/A ‚Äî not checked" lazy shortcuts)

**Time**: 30-90 seconds for full evaluation (OpenAI API + web search latency)

## ‚≠ê Bonus: Hidden Risk Detection

The agent checks 6 categories of hidden risks that standard comparisons miss:

1. **üîß Maintainer/Team Health** ‚Äî GitHub commit patterns, bus factor, contributor churn
2. **üí∞ Pricing Traps** ‚Äî Non-linear cost scaling, hidden fees at volume thresholds
3. **üîí Vendor Lock-in** ‚Äî Proprietary formats, migration difficulty, data export limits
4. **üè¢ Acquisition Risk** ‚Äî Recent M&A, parent company changes, roadmap uncertainty
5. **üìã Compliance Drift** ‚Äî Expired certifications, failed audits
6. **üõ†Ô∏è Technology Deprecation** ‚Äî API sunsets, SDK abandonment

## Files

```
SOUL.md                              # Agent persona + evaluation process + output format
AGENTS.md                            # Skill trigger conditions + execution instructions
orchestrator.py                      # Main coordination logic (4-phase pipeline)
agents/
  candidate_identifier.py            # Phase 1: Find 3-5 candidates
  researcher.py                      # Phase 2: Multi-criteria deep research
  advanced_risk_detector.py          # Phase 2b: 6-type hidden risk detection
  weight_adjuster.py                 # Phase 3: Dynamic weight adjustment
  synthesizer.py                     # Phase 4: Final recommendation
integrations/
  clawhub.py                         # ClawHub web search integration
  openai_client.py                   # OpenAI API client
utils/
  query_parser.py                    # Natural language ‚Üí structured context
  logger.py                          # Logging
config.py                            # Configuration (pydantic models)
run_evaluation.py                    # CLI entry point
skill.json                           # OpenClaw skill manifest
skill_handler.py                     # OpenClaw skill handler
```

## How to Test

Send to the Telegram bot:

**Test 1 ‚Äî India context:**
```
evaluate payment gateways for Indian startup with 10K transactions/month
```

**Test 2 ‚Äî US context (same category, different output):**
```
evaluate payment gateways for US enterprise with 500K transactions/month
```

**Test 3 ‚Äî Different category:**
```
evaluate authentication solutions for healthcare startup with 5000 users
```

Each response will include all 8 sections with different weights, different candidates, different discoveries, and different recommendations.

## Evaluation Rubric Mapping

| Rubric Criteria | Weight | How We Address It |
|---|---|---|
| Adaptive Evaluation | 30% | Explicit weight tables with before/after + evidence chains for each adjustment |
| Research Depth | 25% | Web search for GitHub, status pages, G2, pricing, compliance; hidden risk detection |
| Contextual Awareness | 20% | Different starting weights per context; region/stack/domain affect candidate pool |
| Recommendation Quality | 15% | Primary + backup + conditional alternatives; honest trade-offs with evidence |
| Reproducibility | 10% | Date-stamped; data sources listed; can re-run with updated data |

## Setup

1. OpenClaw on Hostinger VPS with GPT-4o
2. Telegram bot gateway
3. Place SOUL.md and AGENTS.md in OpenClaw workspace
4. Place Python files in workspace/vendor-evaluation/

## Channel

Telegram
